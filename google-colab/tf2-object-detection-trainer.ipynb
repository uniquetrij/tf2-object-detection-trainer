{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf2-object-detection-trainer-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4MBfTBWpHxZr",
        "QdotsqMWM8oJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD7Yq_-ey1il"
      },
      "source": [
        "#@title Give a name to your project\n",
        "TARGET = \"object-detection\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLHM_lNd2716"
      },
      "source": [
        "#@title Where to get your pretrained model from?\n",
        "MODEL_DOWNLOAD_LINK = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d7_coco17_tpu-32.tar.gz\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRmMYWCC3NUx"
      },
      "source": [
        "#@title Where to get your annotated dataset from?\n",
        "DATASET_DOWNLOAD_LINK = \"http://stgrog.ddns.net:8050/pepsico/pepsico.tar.gz\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcXt_tsTCr6F"
      },
      "source": [
        "DRIVE_MOUNTPOINT = \"/content/drive\"\n",
        "\n",
        "def mount_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount(DRIVE_MOUNTPOINT, force_remount=True)\n",
        "\n",
        "_ = !cat {DRIVE_MOUNTPOINT}\n",
        "if 'No such file or directory' in _[0]:\n",
        "    mount_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRrgnMs6l4Vz"
      },
      "source": [
        "# Workspace Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdJrLEwQZNhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e26c6f-0ac7-4b5e-f893-4638fa83f4ac"
      },
      "source": [
        "WORKSPACE = \"tf2-object-detection-trainer\"\n",
        "\n",
        "%mkdir -p /content/{WORKSPACE}\n",
        "%cd /content/{WORKSPACE}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tf2-object-detection-trainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yu8Z3ntkup0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed222b09-3901-4d42-b379-c92463a04232"
      },
      "source": [
        "!pip install python-util tf_slim==1.1.0 lvis==0.5.3 tensorflow-addons==0.12.1 #scipy==1.6.1 pyyaml==5.4.1 gin-config==0.4.0 pandas==1.2.2 matplotlib==3.3.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-util\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/b3/b2a65ab7c591139193bb4f1fa2c870b0d833a3b28ad2c65a1b3e505f2d17/python_util-1.2.1-py3-none-any.whl\n",
            "Collecting tf_slim==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 30.4MB/s \n",
            "\u001b[?25hCollecting lvis==0.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\n",
            "Collecting tensorflow-addons==0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim==1.1.0) (0.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (1.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (1.19.5)\n",
            "Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (0.29.22)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (0.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (4.1.2.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (2.8.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from lvis==0.5.3) (3.2.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.12.1) (2.7.1)\n",
            "Installing collected packages: python-util, tf-slim, lvis, tensorflow-addons\n",
            "Successfully installed lvis-0.5.3 python-util-1.2.1 tensorflow-addons-0.12.1 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfM2rVjfbbKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419797bc-e5df-4ea0-cf2c-eb58b0812513"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models tensorflow/models 2> /dev/null\n",
        "!(cd tensorflow/models;git checkout 73ce096)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: checking out '73ce096'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 73ce096c Control model's input size by parameters instead of hardcoding it in export library.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTIUllZLkNw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a23c12e-7ce4-45b0-b096-3326741f5712"
      },
      "source": [
        "!wget https://github.com/protocolbuffers/protobuf/releases/download/v3.13.0/protoc-3.13.0-linux-x86_64.zip -c\n",
        "!unzip -u protoc-3.13.0-linux-x86_64.zip -d protoc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 17:41:17--  https://github.com/protocolbuffers/protobuf/releases/download/v3.13.0/protoc-3.13.0-linux-x86_64.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/23357588/a595da00-de51-11ea-9242-968f4fc5b907?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210408%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210408T174117Z&X-Amz-Expires=300&X-Amz-Signature=f14af59a8dbd4b1b5cbca11d1f8b4071df02bc19a34ba5e87f9bac06d83734fb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=23357588&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.13.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-04-08 17:41:17--  https://github-releases.githubusercontent.com/23357588/a595da00-de51-11ea-9242-968f4fc5b907?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210408%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210408T174117Z&X-Amz-Expires=300&X-Amz-Signature=f14af59a8dbd4b1b5cbca11d1f8b4071df02bc19a34ba5e87f9bac06d83734fb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=23357588&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.13.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.111.154, 185.199.109.154, 185.199.108.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.111.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1629494 (1.6M) [application/octet-stream]\n",
            "Saving to: ‘protoc-3.13.0-linux-x86_64.zip’\n",
            "\n",
            "protoc-3.13.0-linux 100%[===================>]   1.55M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-04-08 17:41:17 (46.1 MB/s) - ‘protoc-3.13.0-linux-x86_64.zip’ saved [1629494/1629494]\n",
            "\n",
            "Archive:  protoc-3.13.0-linux-x86_64.zip\n",
            "   creating: protoc/include/\n",
            "   creating: protoc/include/google/\n",
            "   creating: protoc/include/google/protobuf/\n",
            "  inflating: protoc/include/google/protobuf/wrappers.proto  \n",
            "  inflating: protoc/include/google/protobuf/field_mask.proto  \n",
            "  inflating: protoc/include/google/protobuf/api.proto  \n",
            "  inflating: protoc/include/google/protobuf/struct.proto  \n",
            "  inflating: protoc/include/google/protobuf/descriptor.proto  \n",
            "  inflating: protoc/include/google/protobuf/timestamp.proto  \n",
            "   creating: protoc/include/google/protobuf/compiler/\n",
            "  inflating: protoc/include/google/protobuf/compiler/plugin.proto  \n",
            "  inflating: protoc/include/google/protobuf/empty.proto  \n",
            "  inflating: protoc/include/google/protobuf/any.proto  \n",
            "  inflating: protoc/include/google/protobuf/source_context.proto  \n",
            "  inflating: protoc/include/google/protobuf/type.proto  \n",
            "  inflating: protoc/include/google/protobuf/duration.proto  \n",
            "   creating: protoc/bin/\n",
            "  inflating: protoc/bin/protoc       \n",
            "  inflating: protoc/readme.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdzRqiGFkkGr"
      },
      "source": [
        "from os import environ as export\n",
        "_ = !pwd\n",
        "export['PATH'] += ':' + _[0] + '/protoc/bin'\n",
        "!(cd tensorflow/models/research;protoc object_detection/protos/*.proto --python_out=.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGVosjt2k2Y8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53379976-e687-48a3-810d-4b6405d54407"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi cocoapi 2> /dev/null\n",
        "!(cd cocoapi;git checkout 8c9bcc3)\n",
        "!(cd cocoapi/PythonAPI/; printf 'conda activate {ENV_NAME}\\nmake\\n' | bash -i )\n",
        "!cp -r ./cocoapi/PythonAPI/pycocotools ./tensorflow/models/research/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: checking out '8c9bcc3'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 8c9bcc3 Merge pull request #183 from wenting-zhao/master\n",
            "bash: cannot set terminal process group (58): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "E}\n",
            "bash: conda: command not found\n",
            "\u001b[01;34m/content/tf2-object-detection-trainer/cocoapi/PythonAPI\u001b[00m# make\n",
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/tf2-object-detection-trainer/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n",
            "\u001b[01;34m/content/tf2-object-detection-trainer/cocoapi/PythonAPI\u001b[00m# exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7N5lAZkrnEH"
      },
      "source": [
        "_ = !pwd\n",
        "TF_OD_PATH = _[0]+'/tensorflow/models/research'\n",
        "\n",
        "from os import environ as export\n",
        "\n",
        "export['PYTHONPATH']=TF_OD_PATH+'/:'\n",
        "export['PYTHONPATH']+=TF_OD_PATH+'/../:'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMZ3od1FGoYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d6c547-3de1-4814-8baf-ff4fedb623ac"
      },
      "source": [
        "!python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 17:41:24.894451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:41:26.452862: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 17:41:26.453890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-08 17:41:26.517338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:26.517940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-04-08 17:41:26.517981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:41:26.635876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 17:41:26.635962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 17:41:26.786538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 17:41:26.825488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 17:41:27.067835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 17:41:27.122840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 17:41:27.126671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 17:41:27.126812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:27.127496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:27.130369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 17:41:27.131081: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 17:41:27.131190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:27.131808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-04-08 17:41:27.131840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:41:27.131955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 17:41:27.132046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 17:41:27.132078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 17:41:27.132102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 17:41:27.132124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 17:41:27.132145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 17:41:27.132166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 17:41:27.132247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:27.132882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:27.133434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 17:41:27.135683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:41:31.519551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-08 17:41:31.519598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-04-08 17:41:31.519613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-04-08 17:41:31.525687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:31.526329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:31.526877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:41:31.527405: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-08 17:41:31.527456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13994 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "tf.Tensor(-876.25867, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMaH3-fBGsNG"
      },
      "source": [
        "# Directory Structure Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TTwrFNPGpxl",
        "outputId": "ea7c8527-03b0-47b8-c8d8-ac70c458130b"
      },
      "source": [
        "%mkdir -p /content/{WORKSPACE}/targets\n",
        "%cd /content/{WORKSPACE}/targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tf2-object-detection-trainer/targets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WBrUf_rpJGA"
      },
      "source": [
        "ANNOTATIONS = 'annotations'\n",
        "EXPORTED = 'exported-models'\n",
        "IMAGES = 'images'\n",
        "TEST_DATA = 'test'\n",
        "TRAIN_DATA = 'train'\n",
        "LABEL_MAP = 'label_map.pbtxt'\n",
        "MODELS = 'models'\n",
        "PRETRAINED = '.pre-trained-models'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uANz4hkYpOBl"
      },
      "source": [
        "!mkdir -p {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{ANNOTATIONS}\n",
        "!mkdir -p {TARGET}/{ANNOTATIONS}\n",
        "\n",
        "!echo \"\" >> {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{ANNOTATIONS}/{LABEL_MAP}\n",
        "!ln -sf {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{ANNOTATIONS}/{LABEL_MAP} {TARGET}/{ANNOTATIONS}/{LABEL_MAP}\n",
        "\n",
        "!mkdir -p {TARGET}/{IMAGES}/{TEST_DATA}\n",
        "!mkdir -p {TARGET}/{IMAGES}/{TRAIN_DATA}\n",
        "\n",
        "!mkdir -p {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{EXPORTED}\n",
        "!mkdir -p {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{MODELS}\n",
        "\n",
        "!ln -sn {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{EXPORTED} {TARGET}/{EXPORTED} \n",
        "!ln -sn {DRIVE_MOUNTPOINT}/MyDrive/colab/{WORKSPACE}/{TARGET}/{MODELS} {TARGET}/{MODELS}\n",
        "\n",
        "!mkdir -p {PRETRAINED}\n",
        "!mkdir -p /content/train\n",
        "\n",
        "!cp {TF_OD_PATH}/object_detection/model_main_tf2.py .\n",
        "!cp {TF_OD_PATH}/object_detection/exporter_main_v2.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK8D6FZ1qu_7"
      },
      "source": [
        "# Training Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfrcsWMeH4Qp",
        "outputId": "46d89737-5a3b-4583-db46-2d62f0f19990"
      },
      "source": [
        "def import_data():\n",
        "    !rm -rf {TARGET}/{IMAGES}/dataset\n",
        "    !wget {DATASET_DOWNLOAD_LINK} -O {TARGET}/{IMAGES}/dataset.tar.gz -c\n",
        "    !tar -zxvf {TARGET}/{IMAGES}/dataset.tar.gz -C {TARGET}/{IMAGES} --one-top-level\n",
        "    _ = ! echo {TARGET}/{IMAGES}/dataset/\n",
        "    export[\"dataset\"]=_[0]\n",
        "    !cd $dataset && find . -type f -exec sh -c 'new=$(echo \"{}\" | sed \"s/\\.\\///\" | tr \"/\" \"-\" | tr \" \" \"_\"); mv \"{}\" \"../$new\"' \\;\n",
        "    !rm -rf dataset*\n",
        "    pass\n",
        "\n",
        "import_data()\n",
        "\n",
        "_ = !grep -Ril \"annotation\" {TARGET}/{IMAGES} | wc -l\n",
        "\n",
        "if int(_[0]) < 100:\n",
        "    raise SystemExit('''\n",
        "    Import your training data to {}/{} before continuing. \n",
        "    You should have at least 100 images for suitable training.\n",
        "    You may complete the `import_data()` function above to assist you to import your data. \n",
        "    '''.format(TARGET, IMAGES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 17:41:34--  http://stgrog.ddns.net:8050/pepsico/pepsico.tar.gz\n",
            "Resolving stgrog.ddns.net (stgrog.ddns.net)... 60.243.242.229\n",
            "Connecting to stgrog.ddns.net (stgrog.ddns.net)|60.243.242.229|:8050... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15167244 (14M) [application/gzip]\n",
            "Saving to: ‘object-detection/images/dataset.tar.gz’\n",
            "\n",
            "object-detection/im 100%[===================>]  14.46M  3.65MB/s    in 4.0s    \n",
            "\n",
            "2021-04-08 17:41:39 (3.65 MB/s) - ‘object-detection/images/dataset.tar.gz’ saved [15167244/15167244]\n",
            "\n",
            "./20.jpg\n",
            "./20.xml\n",
            "./7up_10.jpg\n",
            "./7up_10.xml\n",
            "./7up_11.jpg\n",
            "./7up_11.xml\n",
            "./7up_12.jpg\n",
            "./7up_12.xml\n",
            "./7up_13.jpg\n",
            "./7up_13.xml\n",
            "./7up_14.jpg\n",
            "./7up_14.xml\n",
            "./7up_15.jpg\n",
            "./7up_15.xml\n",
            "./7up_16.jpg\n",
            "./7up_16.xml\n",
            "./7up_17.jpg\n",
            "./7up_17.xml\n",
            "./7up_18.jpg\n",
            "./7up_18.xml\n",
            "./7up_19.jpg\n",
            "./7up_19.xml\n",
            "./7up_1.jpg\n",
            "./7up_1.xml\n",
            "./7up_20.jpg\n",
            "./7up_20.xml\n",
            "./7up_21.jpg\n",
            "./7up_21.xml\n",
            "./7up_22.jpg\n",
            "./7up_22.xml\n",
            "./7up_23.jpg\n",
            "./7up_23.xml\n",
            "./7up_24.jpg\n",
            "./7up_24.xml\n",
            "./7up_25.png\n",
            "./7up_25.xml\n",
            "./7up_2.jpg\n",
            "./7up_2.xml\n",
            "./7up_3.jpg\n",
            "./7up_3.xml\n",
            "./7up_4.jpg\n",
            "./7up_4.xml\n",
            "./7up_5.jpg\n",
            "./7up_5.xml\n",
            "./7up_6.jpg\n",
            "./7up_6.xml\n",
            "./7up_7.jpg\n",
            "./7up_7.xml\n",
            "./7up_8.jpg\n",
            "./7up_8.xml\n",
            "./7up_9.jpg\n",
            "./7up_9.xml\n",
            "./aquafina_12.jpg\n",
            "./aquafina_12.xml\n",
            "./aquafina_13.jpg\n",
            "./aquafina_13.xml\n",
            "./aquafina_14.jpg\n",
            "./aquafina_14.xml\n",
            "./aquafina_16.jpg\n",
            "./aquafina_16.xml\n",
            "./aquafina_17.jpg\n",
            "./aquafina_17.xml\n",
            "./aquafina_18.jpg\n",
            "./aquafina_18.xml\n",
            "./aquafina_19.jpg\n",
            "./aquafina_19.xml\n",
            "./aquafina_1.png\n",
            "./aquafina_1.xml\n",
            "./aquafina_20.jpg\n",
            "./aquafina_20.xml\n",
            "./aquafina_22.jpg\n",
            "./aquafina_22.xml\n",
            "./aquafina_23.jpg\n",
            "./aquafina_23.xml\n",
            "./aquafina_24.jpg\n",
            "./aquafina_24.xml\n",
            "./aquafina_25.jpg\n",
            "./aquafina_25.xml\n",
            "./aquafina_26.jpg\n",
            "./aquafina_26.xml\n",
            "./aquafina_27.jpg\n",
            "./aquafina_27.xml\n",
            "./aquafina_28.jpg\n",
            "./aquafina_28.xml\n",
            "./aquafina_29.jpg\n",
            "./aquafina_29.xml\n",
            "./aquafina_2.png\n",
            "./aquafina_2.xml\n",
            "./aquafina_30.jpg\n",
            "./aquafina_30.xml\n",
            "./aquafina_31.jpg\n",
            "./aquafina_31.xml\n",
            "./aquafina_32.jpg\n",
            "./aquafina_32.xml\n",
            "./aquafina_33.jpg\n",
            "./aquafina_33.xml\n",
            "./aquafina_34.JPG\n",
            "./aquafina_34.xml\n",
            "./aquafina_35.jpg\n",
            "./aquafina_35.xml\n",
            "./aquafina_36.jpg\n",
            "./aquafina_36.xml\n",
            "./aquafina_37.jpg\n",
            "./aquafina_37.xml\n",
            "./aquafina_38.jpg\n",
            "./aquafina_38.xml\n",
            "./aquafina_39.jpg\n",
            "./aquafina_39.xml\n",
            "./aquafina_3.jpg\n",
            "./aquafina_3.xml\n",
            "./aquafina_40.jpg\n",
            "./aquafina_40.xml\n",
            "./aquafina_43.jpg\n",
            "./aquafina_43.xml\n",
            "./aquafina_44.jpg\n",
            "./aquafina_44.xml\n",
            "./aquafina_45.jpg\n",
            "./aquafina_45.xml\n",
            "./aquafina_46.jpg\n",
            "./aquafina_46.xml\n",
            "./aquafina_4.jpg\n",
            "./aquafina_4.xml\n",
            "./aquafina_5.jpg\n",
            "./aquafina_5.xml\n",
            "./aquafina_7.jpg\n",
            "./aquafina_7.xml\n",
            "./aquafina_8.png\n",
            "./aquafina_8.xml\n",
            "./aquafina_9.jpg\n",
            "./aquafina_9.xml\n",
            "./diet_pepsi_10.jpg\n",
            "./diet_pepsi_10.xml\n",
            "./diet_pepsi_11.jpg\n",
            "./diet_pepsi_11.xml\n",
            "./diet_pepsi_12.jpg\n",
            "./diet_pepsi_12.xml\n",
            "./diet_pepsi_13.jpg\n",
            "./diet_pepsi_13.xml\n",
            "./diet_pepsi_14.jpg\n",
            "./diet_pepsi_14.xml\n",
            "./diet_pepsi_15.jpg\n",
            "./diet_pepsi_15.xml\n",
            "./diet_pepsi_16.jpg\n",
            "./diet_pepsi_16.xml\n",
            "./diet_pepsi_17.jpg\n",
            "./diet_pepsi_17.xml\n",
            "./diet_pepsi_18.jpg\n",
            "./diet_pepsi_18.xml\n",
            "./diet_pepsi_19.jpg\n",
            "./diet_pepsi_19.xml\n",
            "./diet_pepsi_1.jpg\n",
            "./diet_pepsi_1.xml\n",
            "./diet_pepsi_20.jpg\n",
            "./diet_pepsi_20.xml\n",
            "./diet_pepsi_21.jpg\n",
            "./diet_pepsi_21.xml\n",
            "./diet_pepsi_22.jpg\n",
            "./diet_pepsi_22.xml\n",
            "./diet_pepsi_23.jpg\n",
            "./diet_pepsi_23.xml\n",
            "./diet_pepsi_24.jpg\n",
            "./diet_pepsi_24.xml\n",
            "./diet_pepsi_25.jpg\n",
            "./diet_pepsi_25.xml\n",
            "./diet_pepsi_26.jpg\n",
            "./diet_pepsi_26.xml\n",
            "./diet_pepsi_27.jpg\n",
            "./diet_pepsi_27.xml\n",
            "./diet_pepsi_28.jpg\n",
            "./diet_pepsi_28.xml\n",
            "./diet_pepsi_29.jpg\n",
            "./diet_pepsi_29.xml\n",
            "./diet_pepsi_2.jpg\n",
            "./diet_pepsi_2.xml\n",
            "./diet_pepsi_30.jpg\n",
            "./diet_pepsi_30.xml\n",
            "./diet_pepsi_31.jpg\n",
            "./diet_pepsi_31.xml\n",
            "./diet_pepsi_32.jpg\n",
            "./diet_pepsi_32.xml\n",
            "./diet_pepsi_33.jpg\n",
            "./diet_pepsi_33.xml\n",
            "./diet_pepsi_34.jpg\n",
            "./diet_pepsi_34.xml\n",
            "./diet_pepsi_35.jpg\n",
            "./diet_pepsi_35.xml\n",
            "./diet_pepsi_36.jpg\n",
            "./diet_pepsi_36.xml\n",
            "./diet_pepsi_37.jpg\n",
            "./diet_pepsi_37.xml\n",
            "./diet_pepsi_38.jpg\n",
            "./diet_pepsi_38.xml\n",
            "./diet_pepsi_3.jpg\n",
            "./diet_pepsi_3.xml\n",
            "./diet_pepsi_4.jpg\n",
            "./diet_pepsi_4.xml\n",
            "./diet_pepsi_5.jpg\n",
            "./diet_pepsi_5.xml\n",
            "./diet_pepsi_6.jpg\n",
            "./diet_pepsi_6.xml\n",
            "./diet_pepsi_7.jpg\n",
            "./diet_pepsi_7.xml\n",
            "./diet_pepsi_8.jpg\n",
            "./diet_pepsi_8.xml\n",
            "./diet_pepsi_9.jpg\n",
            "./diet_pepsi_9.xml\n",
            "./mirinda_10.jpg\n",
            "./mirinda_10.xml\n",
            "./mirinda_11.jpg\n",
            "./mirinda_11.xml\n",
            "./mirinda_12.jpg\n",
            "./mirinda_12.xml\n",
            "./mirinda_13.jpg\n",
            "./mirinda_13.xml\n",
            "./mirinda_14.jpg\n",
            "./mirinda_14.xml\n",
            "./mirinda_15.jpg\n",
            "./mirinda_15.xml\n",
            "./mirinda_16.jpg\n",
            "./mirinda_16.xml\n",
            "./mirinda_17.jpg\n",
            "./mirinda_17.xml\n",
            "./mirinda_18.jpg\n",
            "./mirinda_18.xml\n",
            "./mirinda_19.jpg\n",
            "./mirinda_19.xml\n",
            "./mirinda_1.jpg\n",
            "./mirinda_1.xml\n",
            "./mirinda_20.jpg\n",
            "./mirinda_20.xml\n",
            "./mirinda_21.jpg\n",
            "./mirinda_21.xml\n",
            "./mirinda_22.jpg\n",
            "./mirinda_22.xml\n",
            "./mirinda_23.jpg\n",
            "./mirinda_23.xml\n",
            "./mirinda_24.jpg\n",
            "./mirinda_24.xml\n",
            "./mirinda_25.jpg\n",
            "./mirinda_25.xml\n",
            "./mirinda_26.jfif\n",
            "./mirinda_26.webp\n",
            "./mirinda_26.xml\n",
            "./mirinda_27.jpg\n",
            "./mirinda_27.xml\n",
            "./mirinda_28.jpg\n",
            "./mirinda_28.xml\n",
            "./mirinda_29.jpg\n",
            "./mirinda_29.xml\n",
            "./mirinda_2.jpg\n",
            "./mirinda_2.xml\n",
            "./mirinda_4.jpg\n",
            "./mirinda_4.xml\n",
            "./mirinda_6.jpg\n",
            "./mirinda_6.xml\n",
            "./mirinda_7.jpg\n",
            "./mirinda_7.xml\n",
            "./mirinda_8.jpg\n",
            "./mirinda_8.xml\n",
            "./mirinda_9.jpg\n",
            "./mirinda_9.xml\n",
            "./pepsi_zero_sugar_10.jpg\n",
            "./pepsi_zero_sugar_10.xml\n",
            "./pepsi_zero_sugar_11.jpg\n",
            "./pepsi_zero_sugar_11.xml\n",
            "./pepsi_zero_sugar_12.jpg\n",
            "./pepsi_zero_sugar_12.xml\n",
            "./pepsi_zero_sugar_13.jpg\n",
            "./pepsi_zero_sugar_13.xml\n",
            "./pepsi_zero_sugar_14.jpg\n",
            "./pepsi_zero_sugar_14.xml\n",
            "./pepsi_zero_sugar_15.jpg\n",
            "./pepsi_zero_sugar_15.xml\n",
            "./pepsi_zero_sugar_16.jpg\n",
            "./pepsi_zero_sugar_16.xml\n",
            "./pepsi_zero_sugar_17.jpg\n",
            "./pepsi_zero_sugar_17.xml\n",
            "./pepsi_zero_sugar_18.jpg\n",
            "./pepsi_zero_sugar_18.xml\n",
            "./pepsi_zero_sugar_19.jpg\n",
            "./pepsi_zero_sugar_19.xml\n",
            "./pepsi_zero_sugar_1.jpg\n",
            "./pepsi_zero_sugar_1.xml\n",
            "./pepsi_zero_sugar_20.jpg\n",
            "./pepsi_zero_sugar_20.xml\n",
            "./pepsi_zero_sugar_21.jpg\n",
            "./pepsi_zero_sugar_21.xml\n",
            "./pepsi_zero_sugar_22.jpg\n",
            "./pepsi_zero_sugar_22.xml\n",
            "./pepsi_zero_sugar_23.jpg\n",
            "./pepsi_zero_sugar_23.xml\n",
            "./pepsi_zero_sugar_24.jpg\n",
            "./pepsi_zero_sugar_24.xml\n",
            "./pepsi_zero_sugar_25.jpg\n",
            "./pepsi_zero_sugar_25.xml\n",
            "./pepsi_zero_sugar_26.jpg\n",
            "./pepsi_zero_sugar_26.xml\n",
            "./pepsi_zero_sugar_27.jpg\n",
            "./pepsi_zero_sugar_27.xml\n",
            "./pepsi_zero_sugar_28.jpg\n",
            "./pepsi_zero_sugar_28.xml\n",
            "./pepsi_zero_sugar_29.jpg\n",
            "./pepsi_zero_sugar_29.xml\n",
            "./pepsi_zero_sugar_2.jpg\n",
            "./pepsi_zero_sugar_2.xml\n",
            "./pepsi_zero_sugar_30.jpg\n",
            "./pepsi_zero_sugar_30.xml\n",
            "./pepsi_zero_sugar_31.jpg\n",
            "./pepsi_zero_sugar_31.xml\n",
            "./pepsi_zero_sugar_32.jpg\n",
            "./pepsi_zero_sugar_32.xml\n",
            "./pepsi_zero_sugar_33.jpg\n",
            "./pepsi_zero_sugar_33.xml\n",
            "./pepsi_zero_sugar_34.jpg\n",
            "./pepsi_zero_sugar_34.xml\n",
            "./pepsi_zero_sugar_35.jpg\n",
            "./pepsi_zero_sugar_35.xml\n",
            "./pepsi_zero_sugar_36.jpg\n",
            "./pepsi_zero_sugar_36.xml\n",
            "./pepsi_zero_sugar_37.jpg\n",
            "./pepsi_zero_sugar_37.xml\n",
            "./pepsi_zero_sugar_38.jpg\n",
            "./pepsi_zero_sugar_38.xml\n",
            "./pepsi_zero_sugar_39.jpg\n",
            "./pepsi_zero_sugar_39.xml\n",
            "./pepsi_zero_sugar_3.jpg\n",
            "./pepsi_zero_sugar_3.xml\n",
            "./pepsi_zero_sugar_4.jpg\n",
            "./pepsi_zero_sugar_4.xml\n",
            "./pepsi_zero_sugar_5.jpg\n",
            "./pepsi_zero_sugar_5.xml\n",
            "./pepsi_zero_sugar_6.jpg\n",
            "./pepsi_zero_sugar_6.xml\n",
            "./pepsi_zero_sugar_7.jpg\n",
            "./pepsi_zero_sugar_7.xml\n",
            "./pepsi_zero_sugar_8.jpg\n",
            "./pepsi_zero_sugar_8.xml\n",
            "./pepsi_zero_sugar_9.jpg\n",
            "./pepsi_zero_sugar_9.xml\n",
            "./slice_10.jpg\n",
            "./slice_10.xml\n",
            "./slice_11.jpg\n",
            "./slice_11.xml\n",
            "./slice_12.jpg\n",
            "./slice_12.xml\n",
            "./slice_13.jpg\n",
            "./slice_13.xml\n",
            "./slice_14.jpg\n",
            "./slice_14.xml\n",
            "./slice_15.jpg\n",
            "./slice_15.xml\n",
            "./slice_16.jpg\n",
            "./slice_16.xml\n",
            "./slice_17.jpg\n",
            "./slice_17.xml\n",
            "./slice_18.jpg\n",
            "./slice_18.xml\n",
            "./slice_1.jpg\n",
            "./slice_1.xml\n",
            "./slice_2.jpg\n",
            "./slice_2.xml\n",
            "./slice_3.jpg\n",
            "./slice_3.xml\n",
            "./slice_4.jpg\n",
            "./slice_4.xml\n",
            "./slice_5.jpg\n",
            "./slice_5.xml\n",
            "./slice_6.jpg\n",
            "./slice_6.xml\n",
            "./slice_7.jpg\n",
            "./slice_7.xml\n",
            "./slice_8.jpg\n",
            "./slice_8.xml\n",
            "./slice_9.jpg\n",
            "./slice_9.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MBfTBWpHxZr"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPvJOiuAqmgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfbe88b-0faf-4ae5-e97c-8af2b732135b"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/725f22217178537030fde9492a7cdb0a7fff4d80/docs/source/scripts/partition_dataset.py -c       \n",
        "\n",
        "RANDOM_SEED = 10\n",
        "FRACTION_OF_TEST_DATA = 0.1\n",
        "\n",
        "from pyutil import filereplace\n",
        "filereplace(\"partition_dataset.py\", \"import random.*\", \"import random; random.seed({})\".format(RANDOM_SEED))\n",
        "\n",
        "!rm -rf {TARGET}/{IMAGES}/{TRAIN_DATA}/*\n",
        "!rm -rf {TARGET}/{IMAGES}/{TEST_DATA}/*\n",
        "!python partition_dataset.py -x -i {TARGET}/{IMAGES} -r {FRACTION_OF_TEST_DATA}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 17:41:43--  https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/725f22217178537030fde9492a7cdb0a7fff4d80/docs/source/scripts/partition_dataset.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3577 (3.5K) [text/plain]\n",
            "Saving to: ‘partition_dataset.py’\n",
            "\n",
            "partition_dataset.p 100%[===================>]   3.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-08 17:41:43 (63.2 MB/s) - ‘partition_dataset.py’ saved [3577/3577]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr-vYaxQsCnQ"
      },
      "source": [
        "def get_labels():\n",
        "    from xml.dom import minidom\n",
        "    import glob\n",
        "    objects = set()\n",
        "    xmls = glob.glob('{}/{}/*.xml'.format(TARGET,IMAGES))\n",
        "\n",
        "    for xml in xmls:\n",
        "        xmldoc = minidom.parse(xml)\n",
        "        itemlist = xmldoc.getElementsByTagName('name')\n",
        "        for item in itemlist:\n",
        "            objects.add(item.firstChild.nodeValue)\n",
        "    return objects\n",
        "\n",
        "def generate_labemlap():\n",
        "    global labels\n",
        "    \n",
        "    from IPython.core.getipython import get_ipython\n",
        "    shell = get_ipython()\n",
        "    content = \"%%writefile {TARGET}/{ANNOTATIONS}/{LABEL_MAP}\"\n",
        "    labels = list(get_labels())\n",
        "    for i in range(len(labels)):\n",
        "        content+='''\n",
        "item {{\n",
        "  id: {}\n",
        "  name: '{}'\n",
        "}}'''.format(i+1, labels[i])\n",
        "    content+=\"\\n\\n# generate_labemlap()\"\n",
        "    shell.set_next_input(content, replace=True)\n",
        "    print(\"# Template created. Re-run this code-cell to save your labelmap file\")\n",
        "    print(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CapSonnlsOVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26df9988-7ba0-441d-fe04-46f16d18d1db"
      },
      "source": [
        "generate_labemlap()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Template created. Re-run this code-cell to save your labelmap file\n",
            "%%writefile {TARGET}/{ANNOTATIONS}/{LABEL_MAP}\n",
            "item {\n",
            "  id: 1\n",
            "  name: 'pepsi zero sugar'\n",
            "}\n",
            "item {\n",
            "  id: 2\n",
            "  name: 'slice'\n",
            "}\n",
            "item {\n",
            "  id: 3\n",
            "  name: 'aquafina'\n",
            "}\n",
            "item {\n",
            "  id: 4\n",
            "  name: 'mirinda'\n",
            "}\n",
            "item {\n",
            "  id: 5\n",
            "  name: '7up'\n",
            "}\n",
            "item {\n",
            "  id: 6\n",
            "  name: 'pepsi zero sugarw'\n",
            "}\n",
            "item {\n",
            "  id: 7\n",
            "  name: 'diet pepsi'\n",
            "}\n",
            "\n",
            "# generate_labemlap()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGpfuLPesSJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f8a8ed-6223-4a87-80bd-010dc454fcab"
      },
      "source": [
        "# Template created. Re-run this code-cell to save your labelmap file\n",
        "%%writefile {TARGET}/{ANNOTATIONS}/{LABEL_MAP}\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'head'\n",
        "}\n",
        "\n",
        "# generate_labemlap()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting object-detection/annotations/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOKXrZispfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1ef52f-d401-4988-e0c9-14242a33314c"
      },
      "source": [
        "!wget !wget https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/725f22217178537030fde9492a7cdb0a7fff4d80/docs/source/scripts/generate_tfrecord.py -c     \n",
        "\n",
        "!python generate_tfrecord.py -x {TARGET}/{IMAGES}/{TRAIN_DATA} -l {TARGET}/{ANNOTATIONS}/{LABEL_MAP} -o {TARGET}/{ANNOTATIONS}/train.record\n",
        "!python generate_tfrecord.py -x {TARGET}/{IMAGES}/{TEST_DATA} -l {TARGET}/{ANNOTATIONS}/{LABEL_MAP} -o {TARGET}/{ANNOTATIONS}/test.record\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 17:41:44--  http://!wget/\n",
            "Resolving !wget (!wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘!wget’\n",
            "--2021-04-08 17:41:44--  https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/725f22217178537030fde9492a7cdb0a7fff4d80/docs/source/scripts/generate_tfrecord.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6231 (6.1K) [text/plain]\n",
            "Saving to: ‘generate_tfrecord.py’\n",
            "\n",
            "generate_tfrecord.p 100%[===================>]   6.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-08 17:41:44 (66.5 MB/s) - ‘generate_tfrecord.py’ saved [6231/6231]\n",
            "\n",
            "FINISHED --2021-04-08 17:41:44--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 6.1K in 0s (66.5 MB/s)\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 168, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"generate_tfrecord.py\", line 158, in main\n",
            "    tf_example = create_tf_example(group, path)\n",
            "  File \"generate_tfrecord.py\", line 132, in create_tf_example\n",
            "    classes.append(class_text_to_int(row['class']))\n",
            "  File \"generate_tfrecord.py\", line 101, in class_text_to_int\n",
            "    return label_map_dict[row_label]\n",
            "KeyError: '7up'\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 168, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"generate_tfrecord.py\", line 158, in main\n",
            "    tf_example = create_tf_example(group, path)\n",
            "  File \"generate_tfrecord.py\", line 132, in create_tf_example\n",
            "    classes.append(class_text_to_int(row['class']))\n",
            "  File \"generate_tfrecord.py\", line 101, in class_text_to_int\n",
            "    return label_map_dict[row_label]\n",
            "KeyError: 'aquafina'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coH2J-cYkVdz"
      },
      "source": [
        "# Model Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Hr0K3Lsujw"
      },
      "source": [
        "MODEL_NAME = MODEL_DOWNLOAD_LINK[MODEL_DOWNLOAD_LINK.rindex('/')+1:MODEL_DOWNLOAD_LINK.index('.tar.gz')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV2USDs6ubCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ae9951-9d4f-4d47-85d4-38904e58c46a"
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL_NAME}.tar.gz -P {PRETRAINED} -c\n",
        "!tar -zxvf {PRETRAINED}/{MODEL_NAME}.tar.gz -C {PRETRAINED}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 17:41:48--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d7_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.13.240, 2607:f8b0:4004:809::2010\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.13.240|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 394474998 (376M) [application/x-tar]\n",
            "Saving to: ‘.pre-trained-models/efficientdet_d7_coco17_tpu-32.tar.gz’\n",
            "\n",
            "efficientdet_d7_coc 100%[===================>] 376.20M   127MB/s    in 3.0s    \n",
            "\n",
            "2021-04-08 17:41:51 (127 MB/s) - ‘.pre-trained-models/efficientdet_d7_coco17_tpu-32.tar.gz’ saved [394474998/394474998]\n",
            "\n",
            "efficientdet_d7_coco17_tpu-32/\n",
            "efficientdet_d7_coco17_tpu-32/checkpoint/\n",
            "efficientdet_d7_coco17_tpu-32/checkpoint/ckpt-0.data-00000-of-00001\n",
            "efficientdet_d7_coco17_tpu-32/checkpoint/checkpoint\n",
            "efficientdet_d7_coco17_tpu-32/checkpoint/ckpt-0.index\n",
            "efficientdet_d7_coco17_tpu-32/pipeline.config\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/saved_model.pb\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/assets/\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/variables/\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/variables/variables.data-00000-of-00001\n",
            "efficientdet_d7_coco17_tpu-32/saved_model/variables/variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxhcHyU8tuV5"
      },
      "source": [
        "# Pipeline Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jRiJuuOuhDN"
      },
      "source": [
        "!rm -rf {TARGET}/{MODELS}/{MODEL_NAME}/train\n",
        "\n",
        "!mkdir -p {TARGET}/{MODELS}/{MODEL_NAME}/\n",
        "!cp {PRETRAINED}/{MODEL_NAME}/pipeline.config {TARGET}/{MODELS}/{MODEL_NAME}/\n",
        "!cp {PRETRAINED}/{MODEL_NAME}/checkpoint/ckpt-0* {TARGET}/{MODELS}/{MODEL_NAME}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34ZA8L8pD1Y0"
      },
      "source": [
        "BATCH_SIZE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSsVa7awGz3"
      },
      "source": [
        "import sys\n",
        "sys.path.append(TF_OD_PATH)\n",
        "\n",
        "import tensorflow as tf\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos import pipeline_pb2\n",
        "\n",
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
        "\n",
        "with tf.io.gfile.GFile(\"{}/{}/{}/pipeline.config\".format(TARGET,MODELS,MODEL_NAME), \"r\") as f:                                                                                                                                                                                                                                       \n",
        "    text_format.Merge(f.read(), pipeline_config)       \n",
        "\n",
        "pipeline_config.model.ssd.num_classes = len(labels)\n",
        "pipeline_config.train_config.batch_size = BATCH_SIZE\n",
        "pipeline_config.train_config.fine_tune_checkpoint = \"\"+TARGET+\"/\"+MODELS+\"/\"+MODEL_NAME+\"/ckpt-0\"\n",
        "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "pipeline_config.train_config.use_bfloat16 = False\n",
        "pipeline_config.train_input_reader.label_map_path  = \"\"+TARGET+\"/annotations/label_map.pbtxt\"\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = \"\"+TARGET+\"/\"+ANNOTATIONS+\"/train.record\"   \n",
        "pipeline_config.eval_input_reader[0].label_map_path  = \"\"+TARGET+\"/annotations/label_map.pbtxt\"\n",
        "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = \"\"+TARGET+\"/\"+ANNOTATIONS+\"/test.record\"                                                                                                                                                                                     \n",
        "                                                                                                                                                                                                         \n",
        "with tf.io.gfile.GFile(\"{}/{}/{}/pipeline.config\".format(TARGET,MODELS,MODEL_NAME), \"wb\") as f:                                                                                                                                                                                                                       \n",
        "    f.write(text_format.MessageToString(pipeline_config)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjhqMCVIt0Q2"
      },
      "source": [
        "# Training Schedule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "682JeGinqCxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1d06dc15-07b6-4eab-d204-3556f5fe68b2"
      },
      "source": [
        "# MODEL_NAME = ??? \n",
        "\n",
        "if 'MODEL_NAME' not in vars():\n",
        "    MODEL_NAME = !ls -1 {TARGET}/{MODELS}/\n",
        "    if len(MODEL_NAME) != 1:\n",
        "        del MODEL_NAME\n",
        "        raise SystemExit('''\n",
        "Could not determine model name. Please specify above.\n",
        "        ''')\n",
        "    else:\n",
        "        MODEL_NAME = MODEL_NAME[0]\n",
        "\n",
        "MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'efficientdet_d7_coco17_tpu-32'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHdUrFXzp7M"
      },
      "source": [
        "!mkdir -p {TARGET}/{MODELS}/{MODEL_NAME}/train\n",
        "!mount -o bind /content/train/ {TARGET}/{MODELS}/{MODEL_NAME}/train\n",
        "!rm -rf /content/train/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evJ6Xk8517_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bcb54db-69cd-4806-e42e-d2fc2140298c"
      },
      "source": [
        "!python -u model_main_tf2.py --model_dir={TARGET}/{MODELS}/{MODEL_NAME} --pipeline_config_path={TARGET}/{MODELS}/{MODEL_NAME}/pipeline.config --allow_growth | sed '/nan/q'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 17:42:06.005400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:42:09.549648: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 17:42:09.550498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-08 17:42:09.580739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.581318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-04-08 17:42:09.581357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:42:09.586990: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 17:42:09.587079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 17:42:09.588858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 17:42:09.589184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 17:42:09.596462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 17:42:09.597206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 17:42:09.597433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 17:42:09.597550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.598149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.598695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 17:42:09.599120: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 17:42:09.599239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.599802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-04-08 17:42:09.599832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:42:09.599870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 17:42:09.599893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 17:42:09.599915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 17:42:09.599935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 17:42:09.599955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 17:42:09.599975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 17:42:09.599995: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 17:42:09.600063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.600650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:09.601155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 17:42:09.601203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 17:42:10.110667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-08 17:42:10.110724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-04-08 17:42:10.110736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-04-08 17:42:10.110943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:10.111601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:10.112152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 17:42:10.112666: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-08 17:42:10.112712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13994 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0408 17:42:10.114361 139852691920768 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0408 17:42:10.120817 139852691920768 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0408 17:42:10.121010 139852691920768 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "I0408 17:42:10.140208 139852691920768 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0408 17:42:10.140345 139852691920768 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I0408 17:42:10.140416 139852691920768 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I0408 17:42:10.146118 139852691920768 efficientnet_model.py:147] round_filter input=32 output=56\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.259286 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.266068 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.267805 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.268543 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.274039 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.276646 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.281184 139852691920768 efficientnet_model.py:147] round_filter input=32 output=56\n",
            "I0408 17:42:10.281279 139852691920768 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.295434 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.296151 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.297425 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.298116 139852691920768 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0408 17:42:10.520998 139852691920768 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "I0408 17:42:10.521142 139852691920768 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0408 17:42:11.154698 139852691920768 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0408 17:42:11.154865 139852691920768 efficientnet_model.py:147] round_filter input=40 output=72\n",
            "I0408 17:42:11.825504 139852691920768 efficientnet_model.py:147] round_filter input=40 output=72\n",
            "I0408 17:42:11.825679 139852691920768 efficientnet_model.py:147] round_filter input=80 output=144\n",
            "I0408 17:42:12.657472 139852691920768 efficientnet_model.py:147] round_filter input=80 output=144\n",
            "I0408 17:42:12.657635 139852691920768 efficientnet_model.py:147] round_filter input=112 output=200\n",
            "I0408 17:42:13.633174 139852691920768 efficientnet_model.py:147] round_filter input=112 output=200\n",
            "I0408 17:42:13.633369 139852691920768 efficientnet_model.py:147] round_filter input=192 output=344\n",
            "I0408 17:42:15.481039 139852691920768 efficientnet_model.py:147] round_filter input=192 output=344\n",
            "I0408 17:42:15.481213 139852691920768 efficientnet_model.py:147] round_filter input=320 output=576\n",
            "I0408 17:42:15.817742 139852691920768 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
            "I0408 17:42:15.867236 139852691920768 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "2021-04-08 17:42:16.316787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 17:42:45.559686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 17:42:48.068926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "WARNING:tensorflow:From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/model_lib_v2.py:538: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0408 17:42:50.570371 139852691920768 deprecation.py:339] From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/model_lib_v2.py:538: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['object-detection/annotations/train.record']\n",
            "I0408 17:42:50.586656 139852691920768 dataset_builder.py:163] Reading unweighted datasets: ['object-detection/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['object-detection/annotations/train.record']\n",
            "I0408 17:42:50.586863 139852691920768 dataset_builder.py:80] Reading record datasets for input file: ['object-detection/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0408 17:42:50.586954 139852691920768 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0408 17:42:50.587031 139852691920768 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "W0408 17:42:50.593973 139852691920768 deprecation.py:339] From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "WARNING:tensorflow:From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0408 17:42:50.614265 139852691920768 deprecation.py:339] From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0408 17:42:56.833932 139852691920768 deprecation.py:339] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0408 17:43:00.743657 139852691920768 deprecation.py:339] From /content/tf2-object-detection-trainer/tensorflow/models/research/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-04-08 17:43:02.804597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-08 17:43:02.812444: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im87_c_IYxAt"
      },
      "source": [
        "!pip freeze | grep tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk3iVoeNAIgX"
      },
      "source": [
        "while True:pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdotsqMWM8oJ"
      },
      "source": [
        "# Checkpoint Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_5EEsYeORs"
      },
      "source": [
        "# MODEL_NAME = ??? \n",
        "\n",
        "if 'MODEL_NAME' not in vars():\n",
        "    MODEL_NAME = !ls -1 {TARGET}/{MODELS}/\n",
        "    if len(MODEL_NAME) != 1:\n",
        "        del MODEL_NAME\n",
        "        raise SystemExit('''\n",
        "Could not determine model name. Please specify above.\n",
        "        ''')\n",
        "    else:\n",
        "        MODEL_NAME = MODEL_NAME[0]\n",
        "\n",
        "MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_D3_ijI2C3U"
      },
      "source": [
        "import sys\n",
        "sys.path.append(TF_OD_PATH)\n",
        "sys.path.append(TF_OD_PATH+'/../')\n",
        "\n",
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "PATH_TO_CFG = \"{}/{}/{}/pipeline.config\".format(TARGET,MODELS,MODEL_NAME)\n",
        "PATH_TO_CKPT = \"{}/{}/{}\".format(TARGET,MODELS,MODEL_NAME)\n",
        "\n",
        "print('Loading model... ', end='')\n",
        "start_time = time.time()\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore latest checkpoint\n",
        "latest = next(open(\"{}/{}/{}/checkpoint\".format(TARGET,MODELS,MODEL_NAME)))\n",
        "latest = latest[latest.index(\"\\\"\")+1:latest.rindex(\"\\\"\")]\n",
        "print(latest)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(PATH_TO_CKPT, latest)).expect_partial()\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LGDcXkzWSAf"
      },
      "source": [
        "PATH_TO_LABELS = \"{}/{}/{}\".format(TARGET,ANNOTATIONS,LABEL_MAP)\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOIXq3YVWW0V"
      },
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "def download_images():\n",
        "    base_url = 'file:///content/drive/MyDrive/colab/tf2-object-detection-trainer/head-detection/inference/'\n",
        "    filenames = ['image_01.jpg']\n",
        "    image_paths = []\n",
        "    for filename in filenames:\n",
        "        image_path = tf.keras.utils.get_file(fname=filename,\n",
        "                                            origin=base_url + filename,\n",
        "                                            untar=False)\n",
        "        image_path = pathlib.Path(image_path)\n",
        "        image_paths.append(str(image_path))\n",
        "    return image_paths\n",
        "\n",
        "IMAGE_PATHS = download_images()\n",
        "print(IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj238qUGmC1y"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "for image_path in IMAGE_PATHS:\n",
        "\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "\n",
        "    image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "    # Things to try:\n",
        "    # Flip horizontally\n",
        "    # image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "    # Convert image to grayscale\n",
        "    # image_np = np.tile(\n",
        "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "    detections = detect_fn(input_tensor)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    label_id_offset = 1\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes']+label_id_offset,\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=200,\n",
        "            min_score_thresh=.30,\n",
        "            agnostic_mode=False)\n",
        "\n",
        "    plt.figure()\n",
        "    \n",
        "    plt.imshow(image_np_with_detections)\n",
        "    \n",
        "    print('Done')\n",
        "plt.show()\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTXnls1fezFi"
      },
      "source": [
        "# Model Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gMG8LaLiPQ2"
      },
      "source": [
        "# MODEL_NAME = ??? \n",
        "\n",
        "if 'MODEL_NAME' not in vars():\n",
        "    MODEL_NAME = !ls -1 {TARGET}/{MODELS}/\n",
        "    if len(MODEL_NAME) != 1:\n",
        "        del MODEL_NAME\n",
        "        raise SystemExit('''\n",
        "Could not determine model name. Please specify above.\n",
        "        ''')\n",
        "    else:\n",
        "        MODEL_NAME = MODEL_NAME[0]\n",
        "\n",
        "MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYk2_lCafUn6"
      },
      "source": [
        "!python exporter_main_v2.py --input_type=image_tensor --pipeline_config_path={TARGET}/{MODELS}/{MODEL_NAME}/pipeline.config --trained_checkpoint_dir={TARGET}/{MODELS}/{MODEL_NAME} --output_directory={TARGET}/{EXPORTED}/{MODEL_NAME}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}